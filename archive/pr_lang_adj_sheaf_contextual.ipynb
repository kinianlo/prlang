{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinianlo/prlang/blob/main/archive/pr_lang_adj_sheaf_contextual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6629c529-f711-44a1-9f77-8b94484b503f",
      "metadata": {
        "id": "6629c529-f711-44a1-9f77-8b94484b503f"
      },
      "source": [
        "# PR-like models in langauge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb7b9c91-4e6e-457d-bad3-0c283adef49b",
      "metadata": {
        "id": "cb7b9c91-4e6e-457d-bad3-0c283adef49b"
      },
      "source": [
        "### Generate PR-like models with Masked LM "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy inflect torch transformers\n",
        "%cd\n",
        "%rm -rf contextuality/\n",
        "!git clone https://github.com/kinianlo/contextuality.git\n",
        "%cd contextuality\n",
        "!pip install .\n"
      ],
      "metadata": {
        "id": "f7NFno-CfRPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e68ab1f-8fe3-4b5f-9c89-3994220fdc9f"
      },
      "id": "f7NFno-CfRPW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "/root\n",
            "Cloning into 'contextuality'...\n",
            "remote: Enumerating objects: 105, done.\u001b[K\n",
            "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 105 (delta 28), reused 92 (delta 18), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (105/105), 23.62 KiB | 1.82 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "/root/contextuality\n",
            "Processing /root/contextuality\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from Contextuality==0.0.1) (1.19.5)\n",
            "Requirement already satisfied: picos in /usr/local/lib/python3.7/dist-packages (from Contextuality==0.0.1) (2.3.13)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.7/dist-packages (from picos->Contextuality==0.0.1) (1.2.7)\n",
            "Building wheels for collected packages: Contextuality\n",
            "  Building wheel for Contextuality (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Contextuality: filename=Contextuality-0.0.1-py3-none-any.whl size=10504 sha256=eb732c2a405eab9c1605ab48233e20262c00383ffbbd893fd11603f0b82eed2a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cswkg1gr/wheels/36/0b/52/994c91bbc607abf0ca6f99a923275331c7182be226d830070a\n",
            "Successfully built Contextuality\n",
            "Installing collected packages: Contextuality\n",
            "  Attempting uninstall: Contextuality\n",
            "    Found existing installation: Contextuality 0.0.1\n",
            "    Uninstalling Contextuality-0.0.1:\n",
            "      Successfully uninstalled Contextuality-0.0.1\n",
            "Successfully installed Contextuality-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "%rm -rf prlang\n",
        "!git clone https://github.com/kinianlo/prlang.git\n",
        "!cp prlang/adjectives.txt /root/adjectives.txt\n",
        "!cp prlang/verbs.txt /root/verbs.txt"
      ],
      "metadata": {
        "id": "8_o-3RK_pm0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b982a97e-3738-488a-83d0-954b6d3e9470"
      },
      "id": "8_o-3RK_pm0S",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "Cloning into 'prlang'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 13 (delta 3), reused 12 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2933c683-18fb-48c9-9832-1adb0fd3a595",
      "metadata": {
        "id": "2933c683-18fb-48c9-9832-1adb0fd3a595"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import inflect\n",
        "import random\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2e0d0f08-5645-49f0-990c-e0355e7cc9d1",
      "metadata": {
        "id": "2e0d0f08-5645-49f0-990c-e0355e7cc9d1"
      },
      "outputs": [],
      "source": [
        "def get_probs(sentences_raw, options, model, tokenizer, mask_placeholder='_'):\n",
        "    if type(sentences_raw) is str:\n",
        "        sentences_raw = [sentences_raw]\n",
        "    if type(options[0]) is str:\n",
        "        options = [options for s in sentences_raw]\n",
        "    \n",
        "    partition_size = 2**8\n",
        "    n_sentence = len(sentences_raw)\n",
        "    if n_sentence > partition_size:\n",
        "        print(f\"Number of sentences = {n_sentence} > {partition_size}.\")\n",
        "        probs = []\n",
        "        for i in range(0, len(sentences_raw), partition_size):\n",
        "            print(f\"Currently on {i}/{n_sentence}\")\n",
        "            sentences_raw_part = sentences_raw[i:i+partition_size]\n",
        "            options_part = options[i:i+partition_size]\n",
        "            probs += get_probs(sentences_raw_part, options_part, model,\n",
        "                               tokenizer, mask_placeholder=mask_placeholder)\n",
        "        return probs\n",
        "        \n",
        "    # Convert the option words into tokens\n",
        "    options_token = [[tokenizer.tokenize(op)[0] for op in ops] for ops in options]\n",
        "    options_id = [[tokenizer.vocab[op] for op in ops] for ops in options_token]\n",
        "    \n",
        "    # Replace mask placeholders with the mask token used by the given tokenizer\n",
        "    sentences = [s.replace(mask_placeholder, tokenizer.mask_token) for s in sentences_raw]\n",
        "    inputs = tokenizer(sentences, return_tensors='pt', padding=True)\n",
        "    \n",
        "    mask_indices = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)\n",
        "    \n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    \n",
        "    mask_logits = logits[mask_indices]\n",
        "    \n",
        "    probs = [[] for i in range(len(sentences))]\n",
        "    \n",
        "    for i in range(len(mask_indices[0])):\n",
        "        s_idx, m_idx = mask_indices[0][i], mask_indices[1][i]\n",
        "        prob = torch.softmax(mask_logits[i][options_id[s_idx]], dim=-1).detach().numpy()\n",
        "        prob = dict(zip(options[s_idx], prob))\n",
        "        probs[s_idx].append(prob)\n",
        "        \n",
        "    if len(probs) == 1:\n",
        "        probs = probs[0]\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ec4c356f-44a6-4632-be22-7ed614646a41",
      "metadata": {
        "id": "ec4c356f-44a6-4632-be22-7ed614646a41"
      },
      "outputs": [],
      "source": [
        "# Initialise the language model and its tokenizer\n",
        "lm_name = 'bert-base-uncased'\n",
        "mlm = AutoModelForMaskedLM.from_pretrained(lm_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(lm_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a9bb969c-37f6-45c9-8aa4-1c2edb390225",
      "metadata": {
        "id": "a9bb969c-37f6-45c9-8aa4-1c2edb390225"
      },
      "outputs": [],
      "source": [
        "## Playground\n",
        "### Uncomment this cell to play with the get_probs function \n",
        "#outcomes = ['apple', 'strawberry']\n",
        "#observables = ['sweet', 'red', 'round']\n",
        "#intro = f\"There is an {outcomes[0]} and a {outcomes[1]}.\"\n",
        "#contexts = [f\"The _ is {observables[0]} and {observables[1]}.\",\n",
        "#            f\"The _ is {observables[1]} and {observables[2]}.\",\n",
        "#            f\"The _ is {observables[2]} but the other is {observables[0]}.\"]\n",
        "#probs = get_probs([f\"{c}\" for c in contexts], [outcomes for i in range(3)], mlm, tokenizer)\n",
        "#for i, context in enumerate(contexts):\n",
        "#    print(context)\n",
        "#    print(probs[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b95fc7ea-6116-4cb3-ac11-b41ff650b079",
      "metadata": {
        "id": "b95fc7ea-6116-4cb3-ac11-b41ff650b079"
      },
      "source": [
        "## The following handles a batch of examples\n",
        "Examples should be given in a text file where each row has the format:\n",
        "\n",
        "```outcome1 outcome2: observable1 observable2 observable3 observable4 ...```\n",
        "\n",
        "For example,\n",
        "\n",
        "```apple strawberry: sweet red round green big```\n",
        "\n",
        "For each row, all ordered combinations of 3 observables will be considered in the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f165005e-8037-481b-a4e0-cdbfe4ddb9d1",
      "metadata": {
        "id": "f165005e-8037-481b-a4e0-cdbfe4ddb9d1"
      },
      "outputs": [],
      "source": [
        "def process_topics_file(file_name):\n",
        "    topics = []\n",
        "    with open(file_name) as file:\n",
        "        for row in file:\n",
        "            outcomes = [] \n",
        "            observables = []\n",
        "            outcomes_str, observables_str = map(str.strip, row.split(':'))\n",
        "            outcomes = list(map(str.strip, set(outcomes_str.split(','))))\n",
        "            observables = list(map(str.strip, set(observables_str.split(','))))\n",
        "            topics.append((outcomes, observables))\n",
        "    return topics\n",
        "\n",
        "def process_schemas_file(file_name):\n",
        "    schema_options = dict()\n",
        "    schemas = dict()\n",
        "    with open(file_name) as file:\n",
        "        data = json.load(file)\n",
        "        schema_options = data['options']\n",
        "        schemas = data['schemas']\n",
        "    return schema_options, schemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e3a66551-dc12-4050-ad3a-dd8d8d093bc4",
      "metadata": {
        "id": "e3a66551-dc12-4050-ad3a-dd8d8d093bc4"
      },
      "outputs": [],
      "source": [
        "def generate_scenarios(topics, schema):\n",
        "    scenarios = []\n",
        "    if schema == 'adj':\n",
        "        for topic in topics:\n",
        "            outcomes, observables = topic\n",
        "            out_perm = itertools.permutations(outcomes, 2)\n",
        "            obs_perm = itertools.permutations(observables, 3)\n",
        "            scenarios += list(itertools.product(out_perm, obs_perm))\n",
        "    elif schema == 'adj_no_intro':\n",
        "        for topic in topics:\n",
        "            outcomes, observables = topic\n",
        "            out_perm = itertools.combinations(outcomes, 2)\n",
        "            obs_perm = itertools.permutations(observables, 3)\n",
        "            scenarios += list(itertools.product(out_perm, obs_perm))\n",
        "    return scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c0df2481-7e25-4cc5-9ed2-d5d64905f43a",
      "metadata": {
        "id": "c0df2481-7e25-4cc5-9ed2-d5d64905f43a"
      },
      "outputs": [],
      "source": [
        "def a(word):\n",
        "    return inflect.engine().a(word)\n",
        "\n",
        "def generate_sentences(scenario, schema):\n",
        "    sentences = []\n",
        "    o, x = scenario\n",
        "    if schema == 'adj':\n",
        "        intro = f\"There is {a(o[0])} and {a(o[1])}.\"\n",
        "        sentences.append(f\"{intro} The _ is {x[0]} and {x[1]}.\")\n",
        "        sentences.append(f\"{intro} The _ is {x[1]} and {x[2]}.\")\n",
        "        sentences.append(f\"{intro} The _ is {x[2]} and the other one is {x[0]}.\")\n",
        "    elif schema == 'adj_no_intro':\n",
        "        sentences.append(f\"The _ is {x[0]} and {x[1]}.\")\n",
        "        sentences.append(f\"The _ is {x[1]} and {x[2]}.\")\n",
        "        sentences.append(f\"The _ is {x[2]} and the other one is {x[0]}.\")\n",
        "    return sentences\n",
        "\n",
        "def get_mask_options(scenario, schema):\n",
        "    o, x = scenario\n",
        "    return [o for i in range(3)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ac4eff3-3199-449d-843f-900ba47643ab",
      "metadata": {
        "id": "7ac4eff3-3199-449d-843f-900ba47643ab"
      },
      "source": [
        "## Load topics files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f1617f7f-4c69-4402-bd98-68c9732bf359",
      "metadata": {
        "id": "f1617f7f-4c69-4402-bd98-68c9732bf359",
        "outputId": "f2014e31-1eca-451b-b6c9-39a15ba01425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded topics for schema `adj`:\n",
            "(['apple', 'cherry', 'strawberry'], ['rotten', 'sweet', 'sour', 'round', 'red'])\n",
            "(['cat', 'dog'], ['cute', 'furry', 'sweet', 'lovely', 'friendly'])\n",
            "(['potato', 'yam'], ['starchy', 'big', 'orange', 'healthy'])\n",
            "(['cucumber', 'courgette'], ['long', 'green', 'juicy', 'tasty'])\n",
            "(['marigold', 'daisy'], ['small', 'beautiful', 'yellow', 'everywhere'])\n",
            "(['coreopsis', 'daisy', 'sunflower'], ['small', 'beautiful', 'yellow'])\n",
            "(['butterfly', 'moth'], ['colorful', 'light', 'winged', 'beautiful'])\n",
            "(['dolphin', 'porpoise'], ['wet', 'slippery', 'slim', 'grey'])\n",
            "(['car', 'bus'], ['sturdy', 'fast', 'safe', 'heavy'])\n"
          ]
        }
      ],
      "source": [
        "topics = process_topics_file('adjectives.txt') \n",
        "schema = 'adj'\n",
        "\n",
        "print(f'Loaded topics for schema `{schema}`:')\n",
        "for t in topics:\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1778c5cb-d6cb-46ef-be5e-7ce8d6c99791",
      "metadata": {
        "id": "1778c5cb-d6cb-46ef-be5e-7ce8d6c99791",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e3da6d-595e-4b4c-89c2-9a06a52202a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of scenarios: 804\n"
          ]
        }
      ],
      "source": [
        "scenarios = generate_scenarios(topics, schema)\n",
        "\n",
        "# Too many scenarios would require too much computing time\n",
        "# So randomly select a few to continue\n",
        "random.shuffle(scenarios)\n",
        "scenarios = scenarios\n",
        "\n",
        "n_scenario = len(scenarios)\n",
        "print(f\"Number of scenarios: {n_scenario}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4d273715-5ffc-4a39-9b02-9c9806accc55",
      "metadata": {
        "id": "4d273715-5ffc-4a39-9b02-9c9806accc55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64158194-d890-4c43-edae-2b5691f453f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences = 2412 > 128.\n",
            "Currently on 0/2412\n",
            "Currently on 128/2412\n",
            "Currently on 256/2412\n",
            "Currently on 384/2412\n",
            "Currently on 512/2412\n",
            "Currently on 640/2412\n",
            "Currently on 768/2412\n",
            "Currently on 896/2412\n",
            "Currently on 1024/2412\n",
            "Currently on 1152/2412\n",
            "Currently on 1280/2412\n",
            "Currently on 1408/2412\n",
            "Currently on 1536/2412\n",
            "Currently on 1664/2412\n",
            "Currently on 1792/2412\n",
            "Currently on 1920/2412\n",
            "Currently on 2048/2412\n",
            "Currently on 2176/2412\n",
            "Currently on 2304/2412\n"
          ]
        }
      ],
      "source": [
        "sentences = [generate_sentences(scen, schema) for scen in scenarios]\n",
        "sentences_flat = list(itertools.chain.from_iterable(sentences))\n",
        "\n",
        "mask_options = [get_mask_options(scen, schema) for scen in scenarios]\n",
        "mask_options_flat = list(itertools.chain.from_iterable(mask_options))\n",
        "\n",
        "probs_flat = get_probs(sentences_flat, mask_options_flat, mlm, tokenizer)\n",
        "probs = [probs_flat[3*i:3*i+3] for i in range(n_scenario)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "26e8fe38-835b-42b1-ac7c-6f4dc4e286cd",
      "metadata": {
        "id": "26e8fe38-835b-42b1-ac7c-6f4dc4e286cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9acdedc-ebe9-42e5-9501-1b703e5d9b21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('dog', 'cat') ('furry', 'sweet', 'lovely')\n",
            "('dog', 'cat') ('sweet', 'furry', 'friendly')\n",
            "('apple', 'cherry') ('sweet', 'round', 'rotten')\n",
            "('apple', 'cherry') ('rotten', 'round', 'sweet')\n",
            "('apple', 'cherry') ('rotten', 'round', 'sour')\n"
          ]
        }
      ],
      "source": [
        "from contextuality.model import Model, CyclicScenario\n",
        "\n",
        "models = []\n",
        "sfs = []\n",
        "for i in range(n_scenario):\n",
        "    outcomes, observables = scenarios[i]\n",
        "    tri_scenario = CyclicScenario(observables, 2)\n",
        "    o0, o1 = outcomes\n",
        "    x0, x1, x2 = observables\n",
        "    \n",
        "    table = []\n",
        "    table.append([probs[i][0][0][o0], 0, 0, probs[i][0][0][o1]])\n",
        "    table.append([probs[i][1][0][o0], 0, 0, probs[i][1][0][o1]])\n",
        "    table.append([0, probs[i][2][0][o0], probs[i][2][0][o1], 0])\n",
        "    \n",
        "    model = Model(tri_scenario, table)\n",
        "    sfs.append(model.signalling_fraction())\n",
        "    if model.signalling_fraction() < 1/6:\n",
        "        models.append(model)\n",
        "        print(outcomes, observables)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(f\"total number of scenarios: {len(sfs)}.\")\n",
        "plt.hist(sfs, bins=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "EQvtlHZ_w67O",
        "outputId": "6fdd1de6-27c6-49df-98de-688755abfcec"
      },
      "id": "EQvtlHZ_w67O",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of scenarios: 804.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3d4972e0-ae9e-41e1-a1ae-c742c186e7bc",
      "metadata": {
        "id": "3d4972e0-ae9e-41e1-a1ae-c742c186e7bc",
        "outputId": "9e9ac778-3920-42c9-d7df-12a3239c2c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                (0, 0) (0, 1) (1, 0) (1, 1)\n",
            "(furry, sweet) 0.5502 0.0000 0.0000 0.4498\n",
            "(sweet, lovely) 0.5071 0.0000 0.0000 0.4929\n",
            "(lovely, furry) 0.0000 0.5297 0.4703 0.0000\n",
            "\n",
            "0.10044586649798704\n",
            "                  (0, 0) (0, 1) (1, 0) (1, 1)\n",
            "(sweet, furry) 0.5003 0.0000 0.0000 0.4997\n",
            "(furry, friendly) 0.4937 0.0000 0.0000 0.5063\n",
            "(friendly, sweet) 0.0000 0.4644 0.5356 0.0000\n",
            "\n",
            "0.07128006133623599\n",
            "                (0, 0) (0, 1) (1, 0) (1, 1)\n",
            "(sweet, round) 0.4542 0.0000 0.0000 0.5458\n",
            "(round, rotten) 0.5073 0.0000 0.0000 0.4927\n",
            "(rotten, sweet) 0.0000 0.5707 0.4293 0.0000\n",
            "\n",
            "0.14139175412219152\n",
            "                (0, 0) (0, 1) (1, 0) (1, 1)\n",
            "(rotten, round) 0.5723 0.0000 0.0000 0.4277\n",
            "(round, sweet) 0.4704 0.0000 0.0000 0.5296\n",
            "(sweet, rotten) 0.0000 0.5425 0.4575 0.0000\n",
            "\n",
            "0.14457327124249364\n",
            "                (0, 0) (0, 1) (1, 0) (1, 1)\n",
            "(rotten, round) 0.5723 0.0000 0.0000 0.4277\n",
            "(round, sour) 0.4353 0.0000 0.0000 0.5647\n",
            "(sour, rotten) 0.0000 0.4320 0.5680 0.0000\n",
            "\n",
            "0.14457327115294494\n"
          ]
        }
      ],
      "source": [
        "for model in models:\n",
        "    print(model)\n",
        "    print(model.signalling_fraction())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "pr_lang.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}