{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6629c529-f711-44a1-9f77-8b94484b503f",
   "metadata": {},
   "source": [
    "# PR-like models in langauge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b9c91-4e6e-457d-bad3-0c283adef49b",
   "metadata": {},
   "source": [
    "### Generate PR-like models with Masked LM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2933c683-18fb-48c9-9832-1adb0fd3a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import inflect\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0d0f08-5645-49f0-990c-e0355e7cc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(sentences_raw, options, model, tokenizer, mask_placeholder='_'):\n",
    "    if type(sentences_raw) is str:\n",
    "        sentences_raw = [sentences_raw]\n",
    "    if type(options[0]) is str:\n",
    "        options = [options for s in sentences_raw]\n",
    "        \n",
    "    # Convert the option words into tokens\n",
    "    options_token = [[tokenizer.tokenize(op)[0] for op in ops] for ops in options]\n",
    "    options_id = [[tokenizer.vocab[op] for op in ops] for ops in options_token]\n",
    "    \n",
    "    # Replace mask placeholders with the mask token used by the given tokenizer\n",
    "    sentences = [s.replace(mask_placeholder, tokenizer.mask_token) for s in sentences_raw]\n",
    "    inputs = tokenizer(sentences, return_tensors='pt', padding=True)\n",
    "    \n",
    "    mask_indices = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    mask_logits = logits[mask_indices]\n",
    "    \n",
    "    probs = [[] for i in range(len(sentences))]\n",
    "    \n",
    "    for i in range(len(mask_indices[0])):\n",
    "        s_idx, m_idx = mask_indices[0][i], mask_indices[1][i]\n",
    "        prob = torch.softmax(mask_logits[i][options_id[s_idx]], dim=-1).detach().numpy()\n",
    "        prob = dict(zip(options[s_idx], prob))\n",
    "        probs[s_idx].append(prob)\n",
    "        \n",
    "    if len(probs) == 1:\n",
    "        probs = probs[0]\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec4c356f-44a6-4632-be22-7ed614646a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the language model and its tokenizer\n",
    "lm_name = 'bert-base-uncased'\n",
    "mlm = AutoModelForMaskedLM.from_pretrained(lm_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9bb969c-37f6-45c9-8aa4-1c2edb390225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Playground\n",
    "### Uncomment this cell to play with the get_probs function \n",
    "#outcomes = ['apple', 'strawberry']\n",
    "#observables = ['sweet', 'red', 'round']\n",
    "#intro = f\"There is an {outcomes[0]} and a {outcomes[1]}.\"\n",
    "#contexts = [f\"The _ is {observables[0]} and {observables[1]}.\",\n",
    "#            f\"The _ is {observables[1]} and {observables[2]}.\",\n",
    "#            f\"The _ is {observables[2]} but the other is {observables[0]}.\"]\n",
    "#probs = get_probs([f\"{c}\" for c in contexts], [outcomes for i in range(3)], mlm, tokenizer)\n",
    "#for i, context in enumerate(contexts):\n",
    "#    print(context)\n",
    "#    print(probs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fc7ea-6116-4cb3-ac11-b41ff650b079",
   "metadata": {},
   "source": [
    "## The following handles a batch of examples\n",
    "Examples should be given in a text file where each row has the format:\n",
    "\n",
    "```outcome1 outcome2: observable1 observable2 observable3 observable4 ...```\n",
    "\n",
    "For example,\n",
    "\n",
    "```apple strawberry: sweet red round green big```\n",
    "\n",
    "For each row, all ordered combinations of 3 observables will be considered in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f165005e-8037-481b-a4e0-cdbfe4ddb9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_topics_file(file_name):\n",
    "    topics = []\n",
    "    with open(file_name) as file:\n",
    "        for row in file:\n",
    "            outcomes = [] \n",
    "            observables = []\n",
    "            outcomes_str, observables_str = map(str.strip, row.split(':'))\n",
    "            outcomes = list(map(str.strip, set(outcomes_str.split(','))))\n",
    "            observables = list(map(str.strip, set(observables_str.split(','))))\n",
    "            topics.append((outcomes, observables))\n",
    "    return topics\n",
    "\n",
    "def process_schemas_file(file_name):\n",
    "    schema_options = dict()\n",
    "    schemas = dict()\n",
    "    with open(file_name) as file:\n",
    "        data = json.load(file)\n",
    "        schema_options = data['options']\n",
    "        schemas = data['schemas']\n",
    "    return schema_options, schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3a66551-dc12-4050-ad3a-dd8d8d093bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scenarios(topics, schema):\n",
    "    scenarios = []\n",
    "    if schema == 'adj':\n",
    "        for topic in topics:\n",
    "            outcomes, observables = topic\n",
    "            out_perm = itertools.permutations(outcomes, 2)\n",
    "            obs_perm = itertools.permutations(observables, 3)\n",
    "            scenarios += list(itertools.product(out_perm, obs_perm))\n",
    "    elif schema == 'adj_no_intro':\n",
    "        for topic in topics:\n",
    "            outcomes, observables = topic\n",
    "            out_perm = itertools.combinations(outcomes, 2)\n",
    "            obs_perm = itertools.permutations(observables, 3)\n",
    "            scenarios += list(itertools.product(out_perm, obs_perm))\n",
    "    return scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0df2481-7e25-4cc5-9ed2-d5d64905f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(word):\n",
    "    return inflect.engine().a(word)\n",
    "\n",
    "def generate_sentences(scenario, schema):\n",
    "    sentences = []\n",
    "    o, x = scenario\n",
    "    if schema == 'adj':\n",
    "        intro = f\"There is {a(o[0])} and {a(o[1])}.\"\n",
    "        sentences.append(f\"{intro} The _ is {x[0]} and {x[1]}.\")\n",
    "        sentences.append(f\"{intro} The _ is {x[1]} and {x[2]}.\")\n",
    "        sentences.append(f\"{intro} The _ is {x[2]} and the other one is {x[0]}.\")\n",
    "    elif schema == 'adj_no_intro':\n",
    "        sentences.append(f\"The _ is {x[0]} and {x[1]}.\")\n",
    "        sentences.append(f\"The _ is {x[1]} and {x[2]}.\")\n",
    "        sentences.append(f\"The _ is {x[2]} and the other one is {x[0]}.\")\n",
    "    return sentences\n",
    "\n",
    "def get_mask_options(scenario, schema):\n",
    "    o, x = scenario\n",
    "    return [o for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4eff3-3199-449d-843f-900ba47643ab",
   "metadata": {},
   "source": [
    "## Load topics files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1617f7f-4c69-4402-bd98-68c9732bf359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded topics for schema `adj`:\n",
      "(['apple', 'strawberry'], ['round', 'red', 'sweet'])\n",
      "(['cat', 'dog'], ['cute furry lovely friendly sweet'])\n",
      "(['yam', 'potato'], ['healthy', 'orange', 'startchy'])\n",
      "(['courgette', 'cucumber'], ['long', 'green', 'juicy'])\n",
      "(['daisy', 'marigold'], ['small', 'yellow', 'beautiful'])\n",
      "(['coreopsis', 'daisy', 'sunflower'], ['small', 'yellow', 'beautiful'])\n",
      "(['butterfly', 'moth'], ['winged', 'light', 'colorful'])\n",
      "(['porpoise', 'dolphin'], ['wet', 'slippery', 'grey'])\n"
     ]
    }
   ],
   "source": [
    "topics = process_topics_file('adjectives.txt') \n",
    "schema = 'adj'\n",
    "\n",
    "print(f'Loaded topics for schema `{schema}`:')\n",
    "for t in topics:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1778c5cb-d6cb-46ef-be5e-7ce8d6c99791",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = generate_scenarios(topics, schema)\n",
    "\n",
    "# Too many scenarios would require too much computing time\n",
    "# So randomly select a few to continue\n",
    "random.shuffle(scenarios)\n",
    "scenarios = scenarios[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d273715-5ffc-4a39-9b02-9c9806accc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scenario = len(scenarios)\n",
    "\n",
    "sentences = [generate_sentences(scen, schema) for scen in scenarios]\n",
    "sentences_flat = list(itertools.chain.from_iterable(sentences))\n",
    "\n",
    "mask_options = [get_mask_options(scen, schema) for scen in scenarios]\n",
    "mask_options_flat = list(itertools.chain.from_iterable(mask_options))\n",
    "\n",
    "probs_flat = get_probs(sentences_flat, mask_options_flat, mlm, tokenizer)\n",
    "probs = [probs_flat[3*i:3*i+3] for i in range(n_scenario)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26e8fe38-835b-42b1-ac7c-6f4dc4e286cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextuality.model import Model, CyclicScenario\n",
    "\n",
    "models = []\n",
    "for i in range(n_scenario):\n",
    "    outcomes, observables = scenarios[i]\n",
    "    tri_scenario = CyclicScenario(observables, 2)\n",
    "    o0, o1 = outcomes\n",
    "    x0, x1, x2 = observables\n",
    "    \n",
    "    table = []\n",
    "    table.append([probs[i][0][0][o0], 0, 0, probs[i][0][0][o1]])\n",
    "    table.append([probs[i][1][0][o0], 0, 0, probs[i][1][0][o1]])\n",
    "    table.append([0, probs[i][2][0][o0], probs[i][2][0][o1], 0])\n",
    "    \n",
    "    model = Model(tri_scenario, table)\n",
    "    models.append(model)\n",
    "    if model.signalling_fraction() < 1/2 - 1/12:\n",
    "        print(outcomes, observables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d4972e0-ae9e-41e1-a1ae-c742c186e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               (0, 0) (0, 1) (1, 0) (1, 1)\n",
      "(juicy, green) 0.0964 0.0000 0.0000 0.9036\n",
      "(green, long) 0.0680 0.0000 0.0000 0.9320\n",
      "(long, juicy) 0.0000 0.2430 0.7570 0.0000\n",
      "\n",
      "0.864006027554681\n",
      "                 (0, 0) (0, 1) (1, 0) (1, 1)\n",
      "(slippery, grey) 0.9975 0.0000 0.0000 0.0025\n",
      "(grey, wet) 0.9911 0.0000 0.0000 0.0089\n",
      "(wet, slippery) 0.0000 1.0000 0.0000 0.0000\n",
      "\n",
      "0.9999312653788658\n",
      "                    (0, 0) (0, 1) (1, 0) (1, 1)\n",
      "(yellow, small) 0.0029 0.0000 0.0000 0.9971\n",
      "(small, beautiful) 0.0026 0.0000 0.0000 0.9974\n",
      "(beautiful, yellow) 0.0000 0.0001 0.9999 0.0000\n",
      "\n",
      "0.9998066469524941\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    print(model.signalling_fraction())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
